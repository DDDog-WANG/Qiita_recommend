{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# はてなブックマークの記事のurlからhtmlデータを解析する\n",
    "### htmlデータから取れそうな要素\n",
    "\n",
    "URL,公開日、タイトル、記事内のリンク数、はてな関連記事のリンク数、はてなブックマーク数、本文、ヘッダー情報、それぞれのhtmlタグの数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mongodbからcsvファイルを取ってくる\n",
    "import pandas as pd\n",
    "import os\n",
    "def fetchData():\n",
    "   from pymongo import MongoClient\n",
    "   if not os.path.exists(\"./data\"):\n",
    "       os.mkdir(\"./data\")\n",
    "   #url = open(‘dbserver’, ‘r’).read()\n",
    "   # MongoDBに接続し, テキストデータを取得\n",
    "   client = MongoClient(\"mongodb+srv://group_d:group_d@cluster0-1s7pr.mongodb.net/test?retryWrites=true&w=majority\")\n",
    "   collection = client[\"test\"][\"hateb\"]\n",
    "   df = pd.DataFrame.from_dict(list(collection.find())).astype(object)\n",
    "   pd.DataFrame.to_csv(df, \"./data/hateb.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data/hateb.csvがあるひとはいらない\n",
    "fetchData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#テスト用\n",
    "#url=\"http://rootport.hateblo.jp/entry/20110905/1315233706\"\n",
    "#url = \"http://plus14.hateblo.jp/entry/2019/10/26/193256\"\n",
    "#url =\"https://mohritaroh.hateblo.jp/entry/2019/09/14/203000\"\n",
    "#url=\"https://qiita.com/wanwanland/items/ce272419dde2f95cdabc\"\n",
    "#res = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding: utf-8\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "# aタグリストをurlリストに変換\n",
    "def a_to_href(a_tags):\n",
    "    hrefs = list()\n",
    "    for a_tag in a_tags:\n",
    "        hrefs.append(a_tag.get('href'))\n",
    "    return hrefs\n",
    "\n",
    "# はてな関連のurlの数とその他のurlの数をカウント\n",
    "def count_hrefs(hrefs):\n",
    "    count = 0\n",
    "    hatena = 0\n",
    "    for href in hrefs:\n",
    "        o = urlparse(href)\n",
    "        if len(o.scheme) > 0:\n",
    "            count +=1\n",
    "            if re.search('hate(blo|na)',href):\n",
    "                hatena +=1\n",
    "        else:\n",
    "            continue\n",
    "    return {'total':count,'hatena':hatena,'else':count-hatena}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### analyse_dest(url)の説明\n",
    "urlの入力から辞書型データを返す\n",
    "辞書型のデータは\n",
    "\n",
    "+ url : URL  \n",
    "+ published_at : 公開日 \n",
    "+ title : タイトル \n",
    "+ bookmark : はてなブックマーク数\n",
    "+ tags : 記事にひも付いたタグのリスト\n",
    "+ links \n",
    "    +  total : 記事内の合計link数 \n",
    "    +  hatena : はてな関連記事のlink数 \n",
    "    + else : はてな以外の記事のlink数 \n",
    "+ counts\n",
    "    + head: headタグの数 \n",
    "    + meta: metaタグの数 \n",
    "    + link :linkタグの数\n",
    "    + ...\n",
    "+ header : ヘッダー情報\n",
    "+ text : 文章\n",
    "+ html : htmlそのまま "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "#urlの先を解析\n",
    "def analyse_dest(url) :\n",
    "    \n",
    "    #エラー処理\n",
    "    try:\n",
    "        res = requests.get(url)\n",
    "        if res.status_code is not 200:\n",
    "           #print(\"404 not found\")\n",
    "           return\n",
    "        \n",
    "        header = res.headers\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        title = soup.title.text\n",
    "        text = soup.text\n",
    "    \n",
    "        api = 'https://bookmark.hatenaapis.com/count/entry?url={}'.format(url)\n",
    "        res=requests.get(api)\n",
    "        bookmark=res.json()\n",
    "    \n",
    "        tags_html = soup.find_all('a',{'class':'entry-category-link'})\n",
    "        tags = list()\n",
    "        for tag_html in tags_html:\n",
    "            tags.append(tag_html.text)\n",
    "    \n",
    "        links = count_hrefs(a_to_href(soup.find_all('a')))\n",
    "    \n",
    "        counts = dict()\n",
    "        for tag in soup.find_all(True):\n",
    "            counts[tag.name] = counts.get(tag.name,0)+1\n",
    "        \n",
    "        #はてなブログ\n",
    "        #published_at = soup.select_one('div > header > div.date.entry-date.first > a > time').get('datetime')\n",
    "        if soup.find('time') is not None:\n",
    "            published_at = soup.find('time').get('datetime')\n",
    "        else:\n",
    "            published_at=None\n",
    "    \n",
    "    except Exception as e:\n",
    "        #print(\"error:\", e.args)\n",
    "        return \n",
    "    \n",
    "    dic = {'url':url,'published_at':published_at ,'title': title,'bookmark':bookmark,'tags':tags,\n",
    "          'links':links,'counts':counts,'header':header,'text':text,'html':soup}\n",
    "    \n",
    "    return dic\n",
    "\n",
    "#dataframeを一行ずつ処理する\n",
    "def analyse_row(row) :\n",
    "    try:\n",
    "        soup = BeautifulSoup(row['html'], 'html.parser')\n",
    "        title = soup.title.text\n",
    "        text = soup.text\n",
    "    \n",
    "        api = 'https://bookmark.hatenaapis.com/count/entry?url={}'.format(row['url'])\n",
    "        res=requests.get(api)\n",
    "        bookmark=res.json()\n",
    "    \n",
    "        tags_html = soup.find_all('a',{'class':'entry-category-link'})\n",
    "        tags = list()\n",
    "        for tag_html in tags_html:\n",
    "            tags.append(tag_html.text)\n",
    "    \n",
    "        links = count_hrefs(a_to_href(soup.find_all('a')))\n",
    "    \n",
    "        counts = dict()\n",
    "        for tag in soup.find_all(True):\n",
    "            counts[tag.name] = counts.get(tag.name,0)+1\n",
    "        \n",
    "        #はてなブログ\n",
    "        #published_at = soup.select_one('div > header > div.date.entry-date.first > a > time').get('datetime')\n",
    "        if soup.find('time') is not None:\n",
    "            published_at = soup.find('time').get('datetime')\n",
    "        else:\n",
    "            published_at=None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"error:\", e.args)\n",
    "        return \n",
    "    \n",
    "    dic = {'published_at':published_at ,'title': title,'bookmark':bookmark,'tags':tags,\n",
    "          'links':links,'counts':counts,'text':text,'html':soup}\n",
    "    \n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dic_for_df(result):\n",
    "   \n",
    "    try:\n",
    "        #辞書を値をする辞書とhtmlをpop\n",
    "        #dic_header = result.pop('header')\n",
    "        dic_links = result.pop('links')\n",
    "        dic_counts = result.pop('counts')\n",
    "        html= result.pop('html')\n",
    "        \n",
    "        \n",
    "        #辞書を値をする辞書の値を追加\n",
    "        result['link_else']=dic_links['else']\n",
    "        \n",
    "        tag_list=['script','div','link','a','span','p','img','li','h1','h2','h3']\n",
    "        for tag in tag_list:\n",
    "            if tag in dic_counts:\n",
    "                result[tag]= dic_counts[tag]\n",
    "            else:\n",
    "                result[tag]= 0\n",
    "    except Exception as e:\n",
    "        print(\"error:\", e.args)\n",
    "        return \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csvfileの読み込み\n",
    "import pandas as pd\n",
    "df_in = pd.read_csv(\"./data/hateb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe変換用に辞書型データのリストを作成\n",
    "dic_list = list()\n",
    "# header情報が欲しい場合3~4分かかる\n",
    "# for url in df_in['url']:\n",
    "#     result = analyse_dest(url)\n",
    "#     dic_list.append(dic_for_df(result))\n",
    "for index,row in df_in.iterrows():\n",
    "    result = analyse_row(row)\n",
    "    if result is None:\n",
    "        continue\n",
    "    \n",
    "    result['url']=row['url']\n",
    "    result['category']=row['category']\n",
    "    dic_list.append(dic_for_df(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csvfileを作成\n",
    "import pandas as pd\n",
    "import os\n",
    "df=pd.DataFrame(dic_list)\n",
    "if not os.path.exists(\"./data\"):\n",
    "       os.mkdir(\"./data\")\n",
    "df.to_csv('./data/hateb_reshaped2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "def import_content(filepath):\n",
    "    mng_client = pymongo.MongoClient('mongodb+srv://group_d:group_d@cluster0-1s7pr.mongodb.net/test?retryWrites=true&w=majority')\n",
    "    #mng_client = pymongo.MongoClient('localhost', 27017)\n",
    "    mng_db = mng_client[\"test\"] # Replace mongo db name\n",
    "    collection_name = 'hateb_reshaped2' # Replace mongo db collection name\n",
    "    db_cm = mng_db[collection_name]\n",
    "#     cdir = os.path.dirname(__file__)\n",
    "#     file_res = os.path.join(cdir, filepath)\n",
    "\n",
    "    data = pd.read_csv(filepath)\n",
    "    data_json = json.loads(data.to_json(orient='records'))\n",
    "    db_cm.remove()\n",
    "    db_cm.insert(data_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"./data/hateb_reshaped2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ito/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: DeprecationWarning: remove is deprecated. Use delete_one or delete_many instead.\n",
      "/Users/ito/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: DeprecationWarning: insert is deprecated. Use insert_one or insert_many instead.\n"
     ]
    }
   ],
   "source": [
    "#mongodbのcsvファイルを送信\n",
    "import_content(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
